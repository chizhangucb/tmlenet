---
title: "Using tmlenet R Package for Estimation of Causal Effects in Observational Network Data"
author: "Oleg Sofrygin and Mark J. van der Laan"
date: "`r Sys.Date()`"
header-includes: \usepackage{graphicx}
output:
  pdf_document:
    toc: true
    toc_depth: 1
    number_sections: true
    highlight: haddock
    keep_tex: true
    latex_engine: pdflatex
fontsize: 10pt
bibliography:
  - TMLENET_simcausal_Rpkgs_2016.bib
vignette: >
  %\VignetteIndexEntry{tmlenet Package: Estimating Causal Effects in Observational Network Data}
  %\VignetteEngine{knitr::rmarkdown}
  \usepackage[utf8]{inputenc}
  \usepackage{float}
---
<!--
    pandoc_args: [
      "--natbib"
    ]
     -->
<!--   - SimCausal_2014.bib
  - R-Pckgs.bib
  - TMLE_networks_2014.bib -->
<!-- graphics: yes -->
<!--  -->
<!--  biblatex -->
<!-- output: rmarkdown::html_vignette -->
<!-- output:
  html_document:
    toc: true
    fig_caption: true
    fig_retina: NULL
    theme: united
    highlight: haddock -->
<!-- output:
  pdf_document:
    toc: true
    highlight: haddock
    keep_tex: true
    latex_engine: xelatex -->
<!-- highlight: zenburn -->
<!-- “default”, “tango”, “pygments”, “kate”, “monochrome”, “espresso”, “zenburn”, and “haddock”  -->

```{r, include=FALSE, results='hide'}
  require(methods)
  require(knitr)
  exportTexOnly=FALSE # do not include any chunks in the final result (R code is still evaluated)
  cache_opt <- TRUE
  opts_chunk$set(fig.path='figure/beamer-',fig.align='center',fig.show='hold',size='footnotesize')
  # to crop white space in output figures:
  knit_hooks$set(pdfcrop = hook_pdfcrop)
  # if(opts_knit$get("rmarkdown.pandoc.to")=="latex") knit_hooks$set(plot=hook_plot_tex)
  # To disable syntax color highlighing of R code in the entire document
  # opts_chunk$set(highlight=FALSE)
  # To change the background color on knitR output from default gray to white:
  # opts_chunk$set(background='white')
  opts_chunk$set(include=!exportTexOnly)
  # options(width=80)  # make the printing fit on the page
  options(width=90)  # make the printing fit on the page
  set.seed(1121)   # make the results repeatable
```

```{r, include=FALSE, eval=FALSE, results='hide'}
  # Need to first load that packages that need to be cited:
  # require(tmlenet)
  # require(simcausal)
  # require(data.table)
  # require(ggplot2)
  # require(ltmle)
  # require(igraph)
  # require(lavaan); require(lavaan.survey); require(sem); require(semPLS); require(OpenMx); require(simsem)
  # require(gems); require(aftgee); require(survsim)
  # # # Select packages to cite:
  # citPkgs <- names(sessionInfo()$otherPkgs)
  # citPkgs <- c(citPkgs, "base")
  # require(knitr)
  # # Write the bibtex file:
  # write_bib(citPkgs, file = "R-Pckgs_tmp.bib")
```

<!-- ************************************************ -->
# Introduction
<!-- ************************************************ -->

This vignette describes how to use \texttt{tmlenet} \textbf{R} package [@R-tmlenet] to estimate the effects of single time point stochastic interventions using such data. The `tmlenet` R package performs estimation of average causal effects for single time point interventions in network-dependent (non-iid) data in the presence of interference and/or spillover. Currently implemented estimation algorithms are the targeted maximum likelihood estimation (TMLE), Horvitz-Thompson or the inverse-probability-of-treatment (IPTW) estimator and the parametric G-computation estimator. The user-specified interventions can be either static, dynamic or stochastic. Asymptotically correct influence-curve-based confidence intervals are also constructed for the TMLE and IPTW. For more information behind the methods implemented the `tmlenet` package, see @sofryginTechreport and @vdL2014nets.

The network data examples in this vignette were simulated with the `simcausal` R package [@R-simcausal]. To learn more about the general functionality of the 
`simcausal` package, see the corresponding vignette titled: "simcausal Package: Simulations with Complex Longitudinal Data". To learn more about using the `simcausal` package specifically for conducting the network-based simulations, see the vignette "simcausal Package: Simulations with Networks-Based Dependent-Data Structural Equation Models". 

## Targeted Maximum Likelihood Estimation for Network Data

The \textbf{tmlenet} R package implements the Targeted Maximum Likelihood Estimation (TMLE) of causal effects 
under single time point stochastic interventions in network data. 
The package also implements the Horvitz-Thompson estimator for networks (IPTW) and the parametric g-computation formula estimator. 
The inference for the TMLE is based on its asymptotic normality and the efficient influence curve for dependent data. 
The inference for IPTW is based on its corresponding influence curve for dependent data.

We start by describing the type of data that may arise in an observational study on a population of connected units. Consider a study in which we observe a sample of N dependent units. For each unit we collect baseline covariates, a binary exposure, and a one-dimensional outcome of interest. We denote the sample by the random vector 
$\mathbf{O}=(\mathbf{W},\mathbf{A},\mathbf{Y})\sim P_{0}$, 
where $\mathbf{W}=(W_{i})_{i=1}^{N}$ 
is a vector of baseline covariates across all units, 
$\mathbf{A}=(A_{i})_{i=1}^{N}$
is a vector of exposures, 
$\mathbf{Y}=(Y_{i})_{i=1}^{N}$
is a vector of outcomes, and $P_{0}$
belongs to a large semi-parametric model. 
We assume each $W_{i}$ has finite support, $A_{i}$ is allowed to be multivariate (can be binary, discrete or continuous) and $Y_{i}$ is either binary (e.g., indicating survival beyond a specific time point, or the success of a particular intervention) or bounded (e.g., a count of the number of times an event of interest has occurred during the follow-up period, or a continuous measure of a biomarker level at the end of the study). 
For each unit $i$ in the sample, we also collect the information on other units in $\{1,\ldots,N\}\backslash\{i\}$ that are connected to (or influence) $i$. These units are referred to as “$i$'s friends”, and this set is denoted by $F_{i}\subseteq\{1,\ldots,N\}$.
It is assumed that $F_{i}$ is recorded at baseline, along with other baseline covariates, and it is assumed fixed. Additionally, we allow $|F_{i}|$,
the number of friends for unit $i$, to vary in $i$,
but assume that this number is bounded by some known global constant $K(N)$, which can be a function of $N$.
The vector $\mathbf{F}=(F_{i})_{i=1}^{N}$ is then referred to as the “network profile” of $\mathbf{O}$.
For example, in an experiment evaluating the effects of introducing a new service to an online social network, for each unit, $F_{i}$ could denote the set of all online friends of $i$, whose exposure status may influence $i$'s outcome. Alternatively, in a study of the effects of early HIV treatment initiation, $F_{i}$ could be the set of all sexual partners of unit $i$.
We allow for the following types of between-unit dependencies: (i) the unit-level exposures can depend on baseline data of itself and other units, and (ii) unit-level outcomes can depend on baseline and exposure data of itself and other units. An important ingredient of our modeling approach is to assume that the dependence of each unit $i$ on other units is fully described by the network. Specifically, we assume that the dependence of $i$'s treatment and outcome on other units is limited to the set of $i$'s friends. A second important ingredient is the assumption that these dependencies can be accurately described with some known summary measures, which map the data collected on friends of each unit into a summary that has the same dimension for all units.

The input data structure consists of rows of unit-specific observations, with each row \texttt{i} represented by random variables
(\texttt{F.i},\texttt{W.i},\texttt{A.i},\texttt{Y.i}), where \texttt{F.i} is a vector of "\emph{friend IDs}" of unit \texttt{i} 
(also referred to as \texttt{i}'s "\emph{network}"), \texttt{W.i} is a vector of \texttt{i}'s baseline covariates, \texttt{A.i} is \texttt{i}'s exposure
(either binary, categorical or continuous) and \texttt{Y.i} is \texttt{i}'s binary outcome.
Each exposure \texttt{A.i} depends on (possibly multivariate) baseline summary measure(s) \texttt{sW.i}, where \texttt{sW.i}
can be any user-specified function of \texttt{i}'s baseline covariates \texttt{W.i} and the baseline covariates of \texttt{i}'s friends in
set \texttt{F.i} (all \texttt{W.j} such that \texttt{j} is in \texttt{F.i}).
Similarly, each outcome \texttt{Y.i} depends on \texttt{sW.i} and (possibly multivariate) summary measure(s) \texttt{sA.i},
where \texttt{sA.i} can be any user-specified function of \texttt{i}'s baseline covariates and exposure (\texttt{W.i},\texttt{A.i}) and the
baseline covariates and exposures of \texttt{i}'s friends (all \texttt{W.j},\texttt{A.j} such that \texttt{j} is in \texttt{i}'s friend set \texttt{F.i}). 


The summary measures (\texttt{sW.i},\texttt{sA.i}) are defined simultaneously for all \texttt{i} with functions 
\texttt{{def.sW}} and \texttt{{def.sA}}.
It is assumed that (\texttt{sW.i},\texttt{sA.i}) have the same dimensionality across \texttt{i}. The function \texttt{{eval.summaries}} 
can be used for evaluating these summary measures. All estimation is performed by calling the \texttt{{tmlenet}} function. 
The vector of friends \texttt{F.i} can be specified either as a single column \texttt{NETIDnode} in the input data (where each \texttt{F.i} is a string
of friend IDs or friend row numbers delimited by the character separator \texttt{sep}) or as a separate input matrix \texttt{NETIDmat} of network IDs
(where each row is a vector of friend IDs or friend row numbers).
Specifying the network as a matrix generally results in significant improvements to run time.

## Installation

To install the `simcausal` and `tmlenet` packages, run the code below: 

```{r, eval=FALSE}
install.packages("simcausal", dependencies = TRUE)
install.packages("tmlenet", dependencies = TRUE)
```

To install the development version of these packages (note, requires the `devtools` package):

```{r, eval=FALSE}
devtools::install_github('osofr/simcausal', build_vignettes = FALSE)
devtools::install_github('osofr/tmlenet', build_vignettes = FALSE)
```


<!-- ************************************************ -->
# Technical details
<!-- ************************************************ -->

## The estimands (statistical parameters)

- Briefly describe the observed data model
- The statistical parameters (estimands) that can be estimated with the \texttt{tmlenet} package include:
    + Sample-average expected outcome under static, dynamic and stochastic intervention. Our target statistical quantity $\psi_{0}$
    is defined as a function of this post-intervention distribution. Specifically, it is an expectation of the sample-average of $N$
    outcomes among dependent units $i=1,\ldots,N$, where the expectation is evaluated with respect to the post-intervention distribution $P_{\mathbf{Q},\mathbf{G}^{*}}$:
    $$\psi_{0}=\Psi(P_{0}^{N})=E_{\mathbf{q}_{0},\mathbf{g}^{*}}\left[\dfrac{1}{N}\sum_{i=1}^{N}Y_{i}^{*}\right].$$
    + Alternatively, one may estimate the contrast of two stochastic interventions (on additivite scale), e.g., the sample-average treatment effect. More specifically, define $\Psi^{\mathbf{G}_{1}^{*}}(P_{0}^{N})$ and $\Psi^{\mathbf{G}_{2}^{*}}(P_{0}^{N})$ as the above target parameter evaluated under two different interventions $\mathbf{G}_{1}^{*}$ and $\mathbf{G}_{2}^{*}$, respectively, where each intervention can be static, dynamic or stochastic. The contrast estimand is defined as 
    $$\Psi^{\mathbf{G}_{1}^{*},\mathbf{G}_{2}^{*}}(P_{0}^{N})=\Psi^{\mathbf{G}_{1}^{*}}(P_{0}^{N})-\Psi^{\mathbf{G}_{2}^{*}}(P_{0}^{N}).$$ 
    The sample-average treatment effect over $N$ connected units is a special case of such estimand $\Psi^{\mathbf{G}_{1}^{*},\mathbf{G}_{2}^{*}}(P_{0}^{N})$, where $\mathbf{G}_{1}^{*}$ is defined by degenerate density $\mathbf{g}_{1}^{*}(\mathbf{1}^{N}\:|\:\mathbf{w})=1$ and $\mathbf{G}_{2}^{*}$ is defined by $\mathbf{g}_{2}^{*}(\mathbf{0}^{N}\:|\:\mathbf{w})=1$, for any $\mathbf{w}\in\mathcal{W}$.

Note that by making additional untestable assumptions, one can interpret $\psi_{0}$ as a causal quantity that measures the sample-average of the expected counterfactual outcomes in a network of $N$ connected units under intervention $\mathbf{G}^{*}$ [@vdL2014nets]. However, for the estimation problem at hand, the causal model plays no further role: even when one does not believe any of the untestable causal assumptions, one might still argue that the above statistical parameter $\psi_{0}$ represents an effect measure of interest controlling for all *measured* confounders.

## The workflow

 The following routines will be generally invoked, in the same order as presented below.
 
 \begin{description}
 \item [\emph{{def.sW}}] This is the first part of the two part specification 
  of the structural equation model for the outcome \texttt{Y}.
  Defines the (multivariate) baseline-covariate-based summary measure functions
  that will be later applied to the input data to derive the (multivariate) summary measures \texttt{sW}.
  Each component \texttt{sW[j]} of \texttt{sW} is defined by an R expression that takes as its input
  unit's baseline covariates and the baseline covariates of unit's friends.
  Each argument passed to \texttt{def.sW} is considered a separate summary measure, with the \texttt{j}th argument
  defining the \texttt{j}the summary measure \texttt{sW[j]} and the name of the \texttt{j}th argument defining the name
  of the summary measure \texttt{sW[j]}.
  The arguments of \texttt{def.sW} can be either named, unnamed or a mixture of both. When the argument \texttt{j} is unnamed, 
  the summary measure name for \texttt{sW[j]} is created automatically.
  Each summary measure is defined either by an evaluable R expressions or by a string containing an evaluable R
  expression.
  These expressions can use a special double-square-bracket subsetting operator \texttt{"Var[[index]]"}, which enables
  referencing the variable \texttt{Var} values of unit's friends.
  For example,
  \texttt{Var[[1]]} will evaluate to a one-dimensional vector of summary measures of length \texttt{nrow(data)}, where for each
  row from the input \texttt{data},
  this summary measure will contain the \texttt{Var} value of the unit's first friend. The ordering of friends is
  determined by the ordering of friend IDs specified in the network input. 
  In cases when the unit doesn't have any friends, its corresponding value of \texttt{Var[[1]]} will evaluate
  to \texttt{NA} by default. However, all such \texttt{NA}'s can be replaced by 0's by passing \texttt{replaceNAw0 = TRUE}
  as an additional argument to \texttt{def.sW}.
  One can also use vectors for indexing friend variable \texttt{Var} values in \texttt{Var[[index]]}.
  For example, \texttt{Var[[1:Kmax]]} will evaluate to a \texttt{Kmax}-dimensional summary measure, which will be a matrix
  with \texttt{nrow(data)} rows and \texttt{Kmax} columns,
  where the first column will evaluate to \texttt{Var[[1]]}, the second to \texttt{Var[[2]]}, and so on, 
  up to the last column evaluating to \texttt{Var[[Kmax]]}.
  Note that \texttt{Kmax} is a special reserved constant that can be used inside the network indexing operators.
  It is set to the highest number of friends among all units in the input \texttt{data} and it is specified by
  the user input argument \texttt{Kmax}. See \texttt{def.sW} manual for various examples of 
  summary measures that use the network indexing operators.
\item [\emph{{def.sA}}] Defines treatment summary measures \texttt{sA} that
  can be functions of each unit's exposure and baseline covariates, 
  as well the exposures and baseline covariates of unit's friends. 
  This is the second part of the two part specification of the structural equation model for the outcome \texttt{Y}. 
  The syntax is identical to \texttt{def.sW} function, except that \texttt{def.sA} can consists of functions of baseline covariates
  as well as the exposure \texttt{Anodes}.
\item [\emph{{eval.summaries}}] A convenience function that can be used for 
  validating and evaluating the user-specified summary measures. 
  Takes the input dataset and evaluates the summary measures based on objects previously defined with function calls \texttt{def.sW} and \texttt{def.sA}.
  Note that this function is called automatically by the \texttt{tmlenet} function and does not need to be called by the user prior to calling \texttt{tmlenet}.
\item [\emph{{tmlenet}}] Performs estimation of the causal effect of interest 
  using the observed input \texttt{data}, 
  the intervention of interest, the network information and the previously defined summary measures \texttt{sW}, \texttt{sA}.
\end{description}


## The input data and the network summary measures

The input data are assumed to consist of rows of unit-specific observations, with each row `i` represented by variables (`F.i`,`W.i`,`A.i`,`Y.i`), where `F.i` is a vector of "__friend IDs__" of unit `i` (also referred to as `i`'s "__network__"), `W.i` is a vector of `i`'s baseline covariates, `A.i` is `i`'s exposure (either binary, categorical or continuous) and `Y.i` is `i`'s binary outcome. 

Each exposure `A.i` depends on (possibly multivariate) baseline summary measure(s) `sW.i`, where `sW.i` can be any user-specified function of `i`'s baseline covariates `W.i` and the baseline covariates of `i`'s friends in `F.i` (all `W.j` such that `j` is in `F.i`). Similarly, each outcome `Y.i` depends on `sW.i` and (possibly multivariate) summary measure(s) `sA.i`, where `sA.i` can be any user-specified function of `i`'s baseline covariates and exposure (`W.i`,`A.i`) and the baseline covariates and exposures of `i`'s friends (all `W.j`,`A.j` such that `j` is in `i`'s friend set `F.i`). 

The summary measures (`sW.i`,`sA.i`) are defined simultaneously for all `i` with functions `def.sW` and `def.sA`. It is assumed that (`sW.i`,`sA.i`) have the same dimensionality across `i`. The function `eval.summaries` can be used for evaluating these summary measures.

All estimation is performed by calling the `tmlenet` function. The vector of friends `F.i` can be specified either as a single column in the input data (where each `F.i` is a string of friend IDs or friend row numbers delimited by character `sep`) or as a separate input matrix of network IDs (where each row is a vector of friend IDs or friend row numbers). Specifying the network as a matrix generally results in significant improvements to run time. See `tmlenet` function help file for additional details on how to specify these and the rest of the input arguments.

## Specifying interventions and network

Interventions are specified by passing counterfactual treatment assignment functions as arguments (\texttt{f\_gstar1} and \texttt{optPars\$f\_gstar2}) to the \texttt{tmlenet} function. The functions specified in \texttt{f\_gstar1} and \texttt{optPars\$f\_gstar2}} can only depend on variables specified by the combined matrix of summary measures (\texttt{sW},\texttt{sA}), which is passed using the argument \texttt{data}. These functions should return a vector of length \texttt{nrow(data)} of counterfactual treatments for observations in the input data.


The network of friends (connections) for observations in the input \texttt{data} can be specified in two alternative ways, using either \texttt{NETIDnode} or \texttt{NETIDmat} input arguments of the \texttt{tmlenet} function.
 
\texttt{NETIDnode} - The first (slower) method uses a vector of strings in \texttt{data[, NETIDnode]}, where each
  string \texttt{i} must contain the space separated IDs or row numbers of all units in \texttt{data} thought to be
  connected to observation i (friends of unit i);
 
\texttt{NETIDmat} - An alternative (and faster) method is to pass a matrix with \texttt{Kmax} columns and nrow(data)
  rows, where each row \texttt{NETIDmat[i,]} is a vector of observation \texttt{i}'s friends' IDs or \texttt{i}'s friends'
  row numbers in \texttt{data} if \texttt{IDnode=NULL}. If observation \texttt{i} has fewer than \texttt{Kmax} friends, the
  remainder of \texttt{NETIDmat[i,]} must be filled with \texttt{NA}s. Note that the ordering of friend indices is
  irrelevant.

## Using syntax `'[['` for referencing the network variable values

The `network` function call that defines the network of friends can be added to a growing `DAG` object with syntax `'+network(...)'`, much like a new `node` is added to a growing `DAG` object when sampling iid observations. Subsequently defined nodes (`node` function calls) can employ the double square bracket subsetting syntax `'[['` to reference previously simulated node values for specific friends in $F_i$, simultaneously across all observations `i`. 

For example, using the expression `'VarName[[net_indx]]'` inside the formula of the node function call, will look-up the values of the node `'VarName'` for observations in $F_i[net\_indx]$,  simultaneously across all $i \in 1,\ldots,n$. When `net_indx` is a vector, the evaluation result of the expression `'VarName[[net_indx]]'` is a matrix with `length(net_indx)` columns and `n` rows, and when `net_indx` is a scalar, this evaluation results in a vector. We note that indexing variable `net_indx` above can be a non-negative integer-valued vector, with values starting from 0 and bounded above by the specially reserved integer constant `Kmax`. Using `0` as part of the `net_indx` vector refers to the actual `'VarName'` node values of the observations, hence the expression `'VarName[[0]]'` is equivalent to just using `'VarName'`. When `net_indx` is 1, the expression refers to the node `'VarName'` values for observations in $F_i[1]$, across all $i \in 1,\ldots,n$ (that is, the value of `'VarName'` of `i`'s first friend $F_i[1]$, if the friend exists and `NA` otherwise), and so on, up to `net_indx` value of `Kmax`, which would reference the last friend node values of `VarName`, as defined by observation $F_i[Kmax]$, if `i` has a total of `Kmax` friends and is `NA` otherwise. Also note that the reserved variable name `'Kmax'` can be explicitly used as part of the friend node indexing, i.e., using the node formula expression `'VarName[[Kmax]]'`, will first set the variable `Kmax` to the maximal number of friends observed in a specific network, and then evaluate this node expression according to the rule just described. By default, the expression `'VarName[[k]]'` evaluates to missing (i.e., `'NA'`) for observations that have fewer than `k` friends. This default behavior however can be changed to return `0` instead of `NA`, by passing an additional argument `'replaceNAw0=TRUE'` to the corresponding `node` function call.


Finally, we note that `simcausal` does not allow simultaneous friend references of the same node, that is, each newly added node can be defined only as a function of the nodes that have already been added to the structural equation model. For example, for unit `i`, a node named `Var` cannot depend on the same node `Var` values of `i`'s friends.

## Network-based variable summary measures

One can define the summary measures of the network covariates by specifying a node formula that applies an `R` function to the evaluation result of the expression `Var[[indx]]`, where `indx` is a vector, and the evaluation result of `Var[[indx]]` is a matrix. The rules for defining and applying such summary measures are identical to the rules for defining and applying the summary measures to the evaluation result of the time-varying node, such as the result of the expression `Var[t_indx]`, where `t_indx` is a vector. For example, one can use `sum(Var[[indx]])` to define a summary measure as a sum of `Var` values of friends in $F_i(indx)$, across all observations $i \in 1,\ldots,N$. Similarly, use `mean(Var[[indx]])` to define a summary measure as a mean of `Var` values of friends in $F_i(indx)$, across all $i$. For more details on defining such summary functions see the vignette "simcausal Package: Simulations with Complex Longitudinal Data" and the examples below.

## Estimators

The estimators implemented in this package are described in detail in @sofryginTechreport and are briefly reviewed below.

### Inverse Probability Treatment-Weighted (IPTW):
 
The first step of IPTW is to construct an estimator ${P_{g_N}(A^s | W^s)}$ for common (in $i$) conditional density ${P_{g_0}(A^s | W^s)}$ and common (in $i$) summary measures $(A^s,W^s)$. The same fitting algorithm is then applied to construct an estimator ${P_{g^*_N}(A^{*,s} | W^{*,s})}$ of the common (in $i$) conditional density ${P_{g^*_0}(A^{*,s} | W^{*,s})}$ for common (in $i$) unit-level summary measures $(A^{*,s},sW^{*,s})$ implied by the user-supplied stochastic interventions \texttt{f\_gstar1} or \texttt{f\_gstar2} and the empirical distribution of ${W}$.

These two density estimators form the basis of the IPTW estimator, which is evaluated at the observed N data points ${O_i=(W^s_i, A^s_i, Y_i), i=1,...,N}$ and is given by
$${\psi^{IPTW}_n = \sum_{i=1,...,N}{Y_i \dfrac{P_{g^*_N}(A^{*,s}=A^s_i | W^s=W^s_i)}{P_{g_N}(A^s=A^s_i | W^s=W^s_i)}}.}$$

### G-computation
...

### Targeted Maximum Likelihood
...

### Modeling ${P(A^s|W^s)}$ for summary measures ${(A^s,W^s)}$:

Non-parametric estimation of the common \emph{unit-level} multivariate joint conditional probability model ${P_{g_0}(A^s|W^s)}$, 
for unit-level summary measures ${(A^s,W^s)}$ generated from the observed exposures and baseline covariates 
${(\mathbf{A},\mathbf{W})=(A_i,W_i : i=1,...,N)}$ (their joint density given by ${\mathbf{g}_0(\mathbf{A}|\mathbf{W})\mathbf{Q}(\mathbf{W})}$), is performed by first 
constructing the dataset of $N$ summary measures, ${(A^s_i,W^s_i : i=1,...,N)}$, and then fitting the usual iid MLE 
for the common density ${P_{g_0}(A^s|W^s)}$ based on the pooled N sample of these summary measures.
  
Note that ${A^s}$ can be multivariate, e.g., for $k$ dimensional $A^s$, we have $A^s=(A^s(1),\ldots,A^s(k))$, where each component $A^s(j)$ is univariate it can be either binary, categorical or continuous.
The joint probability model for ${P(sA|W^s)} = {P(A^s(1),\ldots,A^s(k)|W^s)}$ can be factorized as
$${P(A^s(1)|W^s)} * {P(A^s(2)|W^s, A^s(1))} * \ldots * {P(A^s(k)|W^s, A^s(1),...,A^s(k-1))},$$
where each of these conditional probability models is fit by a separate algorithm, which depends on the type of the outcome variable $A^s(j)$.
  
For binary $A^s(j)$, the conditional probability $P(A^s(j)|W^s,A^s(1),...,A^s(j-1))$ is evaluated via a typical logistic
regression model. When $A^s(j)$ is continuous (or categorical), its range will be fist partitioned into $K$ bins and the
corresponding $K^{th}$ bin indicators $({B_1,...,B_K)}$, with each bin indicator ${B_j}$ playing the role of the outcome in a 
separate logistic regression model. Thus, the joint probability ${P(A^|W^s)}$ is then defined by such a tree of binary logistic regressions.
 
For simplicity, we now suppose \texttt{sA} is continuous and univariate and we describe here an algorithm for fitting
${P_{g_0}(A^s | W^s)}$ (the algorithm for fitting ${P_{g^*}(A^{*,s} | W^{*,s})}$ is equivalent, except that exposure $\mathbf{A}$ is replaced with exposure $\mathbf{A}^*$ generated under \texttt{f\_gstar1} or \texttt{f\_gstar2} and 
the predictors $W^s$ from the regression formula \texttt{hform.g0} are replaced with predictors $W^{*,s}$
specified by the regression formula \texttt{hform.gstar}).

The evaluation below utilizes the following fact that: (1) Continuous density 
$f_X$ for random variable $X$ can be written as $f_X(x)=S_X(x)h_X(x)$, 
where $S_X(x)=P(X>x)$ is the survival function for $X$ and $h_X=P(X>x|X>=x)$
is the hazard function for $X$; and (2) The discretized survival function $S_X(x)$ can then be 
written as a product of the hazards for $s<x$: $S_X(x)=\prod_{s<x}h_X(x)$.

\begin{enumerate}
\item Generate a dataset of $N$ observed continuous summary measures ($A^s_i:i=1,\ldots,N$) 
from the observed exposures and baseline covariates on $N$ subjects $((A_i,W_i):i=1,\ldots,N)$,
which also includes each subject`s network information.
\item Divide the range of $A^s$ into intervals $S=(i_1,...,i_M,i_{M+1})$ so that each observed data point
$a^s_i$, for $i=1,\ldots,N$ belongs to some interval from $S$, namely, 
for any $a^s$ there is $k\in{1,...,M}$, such that, $i_k < a^s <= i_{k+1}$.
Let the mapping $B(a^s)\in{1,...,M}$ denote a unique interval in $S$ for 
specific $a^s$ value, such that, $i_{B(a^s)} < a^s <= i_{B(a^s)+1}$.
Let $bw_{B(a^s)}:=i_{B(a^s)+1}-i_{B(a^s)}$ denote the length (bandwidth) 
of the interval $i_{B(a)},i_{B(s^s)+1}$. Next, define the binary 
indicators $b_1,...,b_M$, where $b_j:=I(B(s^s)=j)$, for all $j \leq B(s^s)$ and $b_j$ 
is set to missing for all $j>B(a^s)$ (i.e., we set $b_j$ to missing right 
after the indicator $I(B(s^s)=j)$ jumps from 0 to 1).
Now let $A^s$ denote the random variable for the observed summary measure for one unit
and denote by $(B_1,...,B_M)$ the corresponding random indicators for $A^s$ defined as $B_j := I(B(A^s) = j)$, 
for all $j <= B(A^s)$ and $B_j$ set to missing for all $j>B(A^s)$.

\item Next, for each $j=1,...,M$, fit the logistic regression model estimating the conditional 
probability $P(B_j = 1 | B_{j-1}=0, W^s)$, i.e., at each $j$ this is defined as the conditional 
probability of $B_j$ jumping from 0 to 1 at bin $j$, given that $B_{j-1}=0$ and 
each of these logistic regression models is fit only among the observations that are still at 
risk of having $B_j=1$ with $B_{j-1}=0$.

\item Normalize the above conditional probability of $B_j$ jumping from 0 to 1 by its corresponding 
interval length (bandwidth) $bw_j$ to obtain the discrete conditional hazards 
$h_j(W^s):=P(B_j = 1 | (B_{j-1}=0, W^s) / bw_j$, for each $j$.
For the summary measure $A^s$, the above conditional hazard $h_j(W^s)$ is equal 
to $P(A^s \in (i_j,i_{j+1}) | A^s>=i_j, W^s)$, i.e., this is the probability 
that $A^s$ falls in the interval $(i_j,i_{j+1})$, conditional on $W^s$ and conditional on the fact that
$A^s$ does not belong to any intervals before $j$.

\item  Finally, for any given data-point $(a^s,w^s)$, evaluate the discretized 
conditional density for $P(A^s=a^s|W^s=w^s)$ by first evaluating the interval number 
$k=B(a^s)\in{1,...,M}$ for $a^s$ and then computing $\prod_{j=1,...,k-1}{1-h_j(W^s))*h_k(W^s)}$,
which is equivalent to the joint conditional probability that $a^s$ belongs to the 
interval $(i_k,i_{k+1})$ and does not belong to any of the intervals $1$ to $k-1$, conditional on $W^s$.
\end{enumerate}


###Three methods for defining bin (interval) cuttoffs for a continuous one-dimenstional summary measure $A^s(j)$.

There are 3 alternative methods to defining the bin cutoffs $S=(i_1,...,i_M,i_{M+1})$ for a univariate continuous summary measure
$A^s(j)$. The choice of which method is used along with other discretization parameters (e.g., total number of
bins) is controlled via the tmlenet_options() function. See \texttt{?tmlenet\_options} argument \texttt{bin.method} for
additional details.

Approach 1 (\texttt{equal.len}): equal length, default.
The bins are defined by splitting the range of the observed $A^s$ into equal length intervals. 
This is the dafault discretization method, set by passing an argument \texttt{bin.method="equal.len"} to 
\texttt{tmlenet\_options} function prior to calling \texttt{tmlenet()}. The intervals will be defined by splitting the
range of $A^s$ into \texttt{nbins} number of equal length intervals, where \texttt{nbins} is another argument
of \texttt{tmlenet\_options()} function. When \texttt{nbins=NA} (the default setting) the actual value of \texttt{nbins}
is computed at run time by taking the integer value (floor) of \texttt{n/maxNperBin},
for \texttt{n} - the total observed sample size and \texttt{maxNperBin=1000} - another argument of
\texttt{tmlenet\_options()} with the default value 1,000.

Approach 2 (\texttt{equal.mass}): data-adaptive equal mass intervals. The intervals are defined 
by splitting the range of $A^s$ into non-equal length data-adaptive intervals that ensures 
that each interval contains around \texttt{maxNperBin} observations.
This interval definition approach can be selected by passing an argument \texttt{bin.method="equal.mass"} to 
\texttt{tmlenet\_options()} prior to calling \texttt{tmlenet()}. The method ensures that an approximately 
equal number of observations will belong to each interval, where that number of observations for each interval
is controlled by setting \texttt{maxNperBin}. The default setting is \texttt{maxNperBin=1000} observations per interval.

Approach 3 (\texttt{dhist}): combination of 1 \& 2. The data-adaptive approach dhist is a mix of 
Approaches 1 \& 2. See @denby2009variations.
This interval definition method is selected by passing an argument \texttt{bin.method="dhist"} to 
\texttt{tmlenet\_options()}  prior to calling \texttt{tmlenet()}.


<!-- ************************************************ -->
# Estimation with `tmlenet` using a running example
<!-- ************************************************ -->

## Data

We will use the sample dataset (`W`=(`W1`,`W2`,`W3`),`A`,`Y`) and the sample network matrix of friend IDs (`F`) that come along with the package:

```{r, message=FALSE}
library('tmlenet')
options(tmlenet.verbose=FALSE)
data(df_netKmax6)
data(NetInd_mat_Kmax6)

head(df_netKmax6,2)
head(NetInd_mat_Kmax6,2)
```

## Estimation

The estimation algorithm assumes that the outcomes in `Y.i` for units `i=1,...,N` are conditionally independent,
given the summary measures defined in `def_sW` and the summary measures defined in `def_sA`.

When no additional assumptions about the conditional independence of outcomes `Y.i` can be made 
(beyond the dependence on the network structure),
one can define the summary measures `sW` and `sA` non-parametrically, e.g., 
for each observation `i`: include in `sW` all baseline covariates of unit `i` and 
all baseline covariates of `i`'s friends; include in `sA` the exposure of unit `i` and
all exposures of `i`'s friends. 

The example below does just that, defining  `sW`:=(`netW1`,`netW2`,`netW3`) and `sA`:=`netA`, 
where `netVar` is a summary measure of dimension `Kmax+1` and includes `Var` values of each 
unit as well as `Var` values of all friends of each unit:

```{r, message=FALSE}
Kmax <- ncol(NetInd_mat_Kmax6)
def_sW <- def.sW(netW1 = W1[[0:Kmax]], netW2 = W2[[0:Kmax]], netW3 = W3[[0:Kmax]])
def_sA <- def.sA(netA = A[[0:Kmax]])
```

Note that the summary measure `nF` (number of friends for each unit) is always added automatically to 
`def.sW` function calls (only once), but not to `def.sA`.

A helper function that can pre-evaluate the above summary measures based on the input data:

```{r, message=FALSE}
eval_res <- eval.summaries(sW = def_sW, sA = def_sA,  Kmax = 6, data = df_netKmax6,
                          NETIDmat = NetInd_mat_Kmax6)
```

The contents of the list returned by `eval.summaries()` includes the matrix of evaluated baseline summaries $W^s$ (variable `sW.matrix`), the matrix of evaluated exposure summaries $A^s$ (variable `sA.matrix`), and the matrix of network IDs (variable `NETIDmat`), all of which can be accessed as shown below:

```{r}
# Matrix of sW (baseline) summaries:
head(eval_res$sW.matrix,2) 
# Matrix of sA (exposure) summaries:
head(eval_res$sA.matrix,2)
# Matrix of the network IDs:
head(eval_res$NETIDmat,2)
```

The observed data summary measures ($A^s,W^s$) and the network are conveniently stored in a single object named \texttt{DatNet.ObsP0}, which is also returned by the function \texttt{eval.summaries}. The object \texttt{DatNet.ObsP0} can be passed on to the \texttt{tmlenet} function as an alternative way to specify the input data with one argument \texttt{DatNet.ObsP0} (see `?tmlenet` for additional details).

```{r, eval=FALSE}
eval_res$DatNet.ObsP0
```

In the example below, we estimate mean population outcome under deterministic intervention that assigns all `A` to 0
(network specified via a matrix of friend IDs).

```{r, message=FALSE}
res1 <- tmlenet(data = df_netKmax6, NETIDmat = NetInd_mat_Kmax6, Kmax = Kmax, 
                sW = def_sW, sA = def_sA,
                Anodes = "A", Ynode = "Y",
                f_gstar1 = 0L, optPars = list(n_MCsims = 1))
res1$EY_gstar1$estimates
res1$EY_gstar1$vars
res1$EY_gstar1$CIs
```

By default, the conditional expectation `E[Y=1|...]` (`Qform` argument) is estimated by including all
summary measures in `sW` and `sA` as predictors in the logistic regression for the outcome `Y`.
Similarly, by default, the observed exposure model `P(sA|sW)` (`hform.g0` argument) is estimated
as the conditional probability of observing the summary measures defined in `sA`, given the summary measures
defined in `sW`. Finally, the intervention exposure model `P(sA^*|sW)` (`hform.gstar` 
argument) is estimated by first replacing all observed exposures in `A` with those generated from
the intervention function specified in `f_gstar1` (new exposures denoted by `A^*`) and then building
the same summary measures defined in `sW` and `sA` using exposures `A^*` instead of `A`
(new summary measures denoted by `sA^*`). By default, the intervention exposure model `P(sA^*|sW)`
will be estimated as the conditional probability of observing the intervention-based summary measures in `sA^*` 
(`sA^*` built with `A^*` using the same summary mappings as in `sA`), given the summary measures defined in `sW`.

One can change this default behavior and use the arguments `Qform`, `hform.g0` and `hform.gstar`
to select a subset of the summary measures in `sW`,`sA` to be included in each of the three models described above.
For example, below we are assuming that the outcomes in `Y` only depend on the summary measures `netA`,`netW2` 
(regression `"Y~netA+netW2"`) and hence the observed exposure model is given by `P(netA|netW2)` (regression `"netA~netW2"`) and we also know that `f_gstar1` defines a static intervention `A^*=1` and hence `sA^*` is degenerate and doesn't depend on any baseline covariates and will be estimated here with a simplified regression model (regression `"netA ~ nF"`):

Note that in this example we are also using the previously evaluated summary data object `DatNet.ObsP0` as an input to `tmlenet`, thus, avoiding the need to specify the arguments (`data`,`NETIDmat`,`Kmax`,`sW`,`sA`) each time.

```{r, message=FALSE}
res2 <- tmlenet(DatNet.ObsP0 = eval_res$DatNet.ObsP0,
                    Anodes = "A", Ynode = "Y", 
                    Qform = "Y ~ netA + netW2",
                    hform.g0 = "netA ~ netW2",
                    hform.gstar = "netA ~ nF",
                    f_gstar1 = 0L, optPars = list(n_MCsims = 1))
res2$EY_gstar1$estimates
res2$EY_gstar1$vars
res2$EY_gstar1$CIs
```

One might be also willing to make dimension reducing assumptions about the dependence of each `Y.i` on its 
network. For example, here we assume that each `Y.i` depends on its network's baseline covariates only
through a sum of its friends' values of `W3` and `Y.i` depends on its network's exposures only through a sum 
of `i`'s friends' interactions `(1-A)*(W2)` (while we assume `Y.i` still depends on `i`'s baseline covariates and
`i`'s exposure):

```{r, message=FALSE}
def_sW <- def.sW(W = c(W1,W2,W3)) +
          def.sW(sum.netW3 = sum(W3[[1:Kmax]]), replaceNAw0=TRUE)

def_sA <- def.sA(A) +
          def.sA(sum.netAW2 = sum((1-A[[1:Kmax]])*W2[[1:Kmax]]), replaceNAw0=TRUE)

eval_res <- eval.summaries(sW = def_sW, sA = def_sA, Kmax = 6, data = df_netKmax6,
                            NETIDmat = NetInd_mat_Kmax6, verbose = FALSE)

res3 <- tmlenet(DatNet.ObsP0 = eval_res$DatNet.ObsP0,
                Anodes = "A", Ynode = "Y",
                Qform = "Y ~ A + sum.netAW2 + W + sum.netW3 + nF",
                hform.g0 = "A + sum.netAW2 ~ sum.netW3",
                hform.gstar = "A + sum.netAW2 ~ sum.netW3",
                f_gstar1 = 0, optPars = list(n_MCsims = 1))
res3$EY_gstar1$estimates
```

Note that the above model specified by `Qform` includes all summary measures in `sW`,`sA`, and hence is equivalent to the default regression model that would have been used if `Qform` was omitted.

One can specify any intervention of interest, for example below we estimate the counterfactual mean outcome under intervention that randomly assigns 20% of the population to exposure `A=1`. Note that we are also increasing the number of Monte-Carlo simulations
from 1 to 100.

```{r, message=FALSE}
f.A_.2 <- function(data, ...) rbinom(n = nrow(data), size = 1, prob = 0.2)
res4 <- tmlenet(data = df_netKmax6, NETIDmat = NetInd_mat_Kmax6, Kmax = Kmax,
                sW = def_sW, sA = def_sA, 
                Anodes = "A", Ynode = "Y", 
                f_gstar1 = f.A_.2, optPars = list(n_MCsims = 100))
res4$EY_gstar1$estimates
```

To estimate the average treatment effect (ATE) for two interventions (static or stochastic), specify the second intervention function using the argument `optPars(f_gstar2 = ...)`. In the example below, the intervention `f_gstar1`
statically sets everyone's exposure to `A=1` and the intervention `f_gstar2` statically sets everyone's exposure to `A=0`:

```{r, message=FALSE}
res5 <- tmlenet(data = df_netKmax6, NETIDmat = NetInd_mat_Kmax6, Kmax = Kmax,
                sW = def_sW, sA = def_sA, Anodes = "A", Ynode = "Y",
                f_gstar1 = 1, optPars = list(f_gstar2 = 0, n_MCsims = 1))
res5$ATE$estimates
```

<!-- ************************************************ -->
# Simulation study with continuous exposure and a single time point stochastic intervention
<!-- ************************************************ -->

In this example we simulate a network of connections on $n$ units, along with the unit-specific data with 3 binary baseline covariates, $W_i = (W_i(1),W_i(2),W_i(3))$, for $i=1,\ldots,n$. In addition, we also simulate the observed unit-specific exposures $sA_i$, which are continuous and are normally distributed when conditioned on $W_i$, with a common (in $i$) conditional density given by $g_0(sA_i|W_i)$ and parameterized as normal with $\sigma^2=1$ and $$\mu(W_i)= 0.98*W_i(1)+0.58*W_i(2)+0.33*W_i(3).$$ We define the stochastic intervention by known (and common in $i$) density $g^*$, that depends on $g_0$. This new (intervened) exposure under stochastic intervention $g^*$ is denoted by random variable $sA^*_i$ and it is set equal to $sA_i+shift$, for known constant $shift>0$, if the following condition holds: $$\exp\{shift * (sA_i - \mu(W_i) + shift/2)\} \geq trunc \tag{*},$$ for known truncation constant $trunc>0$, and otherwise, the intervention is to keep the exposure unchanged, setting: $$sA^*_i = sA_i.$$ Finally, the outcome $Y_i$, for each unit $i$, is assumed to depend on $i$'s baseline covariates in $W_i$ and $i$'s exposure $sA_i$, as well as the baseline covariate values and exposures of $i$'s friends via the following summary measures: $$W^s_i:=1/|F_i|\sum_{j \in F_i}{W_j}\ \mbox{and}\ A^s_i:=1/|F_i|\sum_{j \in F_i}{sA_j}.$$


Our target causal quantity is defined with respect to $n$ $i$-specific counterfactual outcomes $Y^{*}_i$, under stochastic intervention $g^*$. The counterfactual outcomes are generated by first replacing the structural equations for the observed exposures $sA_i$ (defined by $g_0(sA_i|W_i)$), with the new structural equations for generating $sA^*_i$ (defined by the intervention $g^*(sA^*_i|A_i,W_i)$) and then sampling the outcomes $Y^*_i$ from this modified data-generating distribution, where we use the new notation $Y^{*}_i$, instead of the old outcome $Y_i$, to denote the fact that the distribution of the outcome $Y_i$ has been modified by replacing $g_0$ with intervention $g^*$. Our causal quantity is denoted by $\psi_0$ and we define it as the sample-average of unit-specific expected outcomes, i.e., $$\psi_0 := 1/n\sum_{i=1}^n{E[Y^*_i]},$$ which we will evaluate by using the `simcausal` package.

## Specifying the structural equation model for dependent data

In order to perform network-based simulations with the `simcausal` package one has to first define the sampling distribution function for the network. This network can then serve as a backbone for defining the dependent-data structural equation models, in which any variable can be a function of the previously defined variable values of the unit's friends. For additional examples of such network generators, as well as the detailed specifications of how to define such functions, we refer to the `simcausal` package vignette "Simulations with Networks-Based Dependent-Data Structural Equation Models" [@R-simcausal]. We define the network generator by sampling a regular directed graph (all nodes have the same degree `Kmax`) using the `sample_k_regular` function of the `igraph` package [@igraph]. 

We then use the `simcausal` `R` package [@R-simcausal] to define the structural equation model for baseline covariates (`W1`, `W2` and `W3`), then defining the structural equation model for the observed exposure `sA` and, finally, defining the binary outcome `Y` that depends on the network structure of friends exposures [\textbf{PRESENT THE SEM}].

Next, we evaluate $\psi_0$ under intervention $g^*$ via Monte-Carlo simulation by sampling the counterfactual outcome $Y^*$ from the modified data-generating distribution that replaces $g_0$ with $g^*$. In order to do that, we first define an action named `gstar`, that overrides the distribution of the observed node `new.sA`, with a new distribution given by the common in $i$ density $g^*$, which is as described above, i.e., we shift the value of the observed continuous exposure `sA` only if the condition $(*)$ is satisfied for $trunc=4$ and $shift=1$ constants. We then sample a network of $n=50,000$, sample $Y_i^*$, for $i=1,\ldots,n$, based on stochastic intervention $g^*$ and evaluate the Monte-Carlo estimate of $\psi_0$. 

As our final step of the simulation, we used the same data-generating mechanism to obtain replicate observed data samples using the above defined dependent data generating distribution with the observed treatment mechanism $g_0$. The visualize representation of the observed network for 50 samples is shown in Figure \ref{fig:netplot3} (the plot was created by using the `plot.igraph` function in `igraph` package [@igraph]).


```{r, results='hide', message=FALSE, echo=FALSE}
library('igraph')
generate.igraph.k.regular <- function(n, Kmax, ...) {
    if (n < 20) Kmax <- 5
    igraph.reg <- igraph::sample_k_regular(no.of.nodes = n, 
                                        k = Kmax, 
                                        directed = TRUE, 
                                        multiple = FALSE)
    sparse_AdjMat <- simcausal::igraph.to.sparseAdjMat(igraph.reg)
    NetInd_out <- simcausal::sparseAdjMat.to.NetInd(sparse_AdjMat)
    return(NetInd_out$NetInd_k)
  }
```

```{r, results='hide', message=FALSE, echo=FALSE}
library(simcausal)
options(simcausal.verbose=FALSE)
Kmax <- 10
D <- DAG.empty()
D <- D + network("NetInd_k", Kmax = Kmax, netfun = "generate.igraph.k.regular")

D <- D +
    node("W1", distr = "rbern", prob = 0.5) +
    node("W2", distr = "rbern", prob = 0.3) +
    node("W3", distr = "rbern", prob = 0.3) +
    node("sA.mu", distr = "rconst", const = (0.98 * W1 + 0.58 * W2 + 0.33 * W3)) +
    node("sA", distr = "rnorm", mean = sA.mu, sd = 1) +
    node("new.sA", distr = "rconst", const = sA) +
    node("probY", distr = "rconst",
      const = plogis( 
                      -0.35*new.sA +
                      -0.5*ifelse(nF > 0, sum(new.sA[[1:Kmax]])/nF, 0) +
                      -0.5*ifelse(nF > 0, sum(W1[[1:Kmax]])/nF, 0) +
                      -0.5*W1 - 0.58*W2 - 0.33*W3),
      replaceNAw0 = TRUE) +
    node("Y", distr = "rbern", prob = probY)

Dset <- set.DAG(D)
```

```{r, results='hide', message=FALSE, echo=FALSE}
trunc <- 4
shift <- 1

Dset <- Dset + 
  action("gstar", 
    nodes = node("new.sA",
      distr = "rconst", 
      const = ifelse(exp(shift * (sA + shift - sA.mu - shift/2)) > trunc,
                    sA, sA + shift)),
    trunc = trunc, shift = shift)
```

```{r, results='hide', message=FALSE, echo=FALSE}
`%+%` <- function(a, b) paste0(a, b)
nfull <- 50000
datFull <- sim(Dset, actions="gstar", n = nfull, rndseed = 54321)[[1]]
psi0 <- mean(datFull$Y)
print(c("psi0: ", psi0))
```

```{r, results='hide', message=FALSE, echo=FALSE}
nsamp <- 50
datO <- sim(Dset, n = nsamp, rndseed = 54321)
print("mean(datO$Y): " %+% mean(datO$Y));

NetInd_mat <- attributes(datO)$netind_cl$NetInd
nF <- attributes(datO)$netind_cl$nF
datO <- datO[,c("W1", "W2", "W3", "sA", "Y")]
print(head(datO,2))
print(head(NetInd_mat,2))
```


```{r netplot3, fig.width=7, fig.height=5, message=FALSE, echo=FALSE, fig.cap = "Directed random regular graph (all nodes have the same degree) from Example 3"}
g <- sparseAdjMat.to.igraph(NetInd.to.sparseAdjMat(NetInd_mat, nF = nF))
par(mar=c(.1,.1,.1,.1))
plot.igraph(g,
    layout=layout.circle,
    vertex.size=5,
    vertex.label.cex=.5,
    edge.arrow.size=.1)
```

<!-- Similarly, we can plot the functional relationships between the nodes in the `DAG` object `Dset`, as shown below. -->
<!-- {r DAGplot3, fig.width=6, fig.height=5, message=FALSE, fig.cap = "Functional relationships between the nodes of the structural equation model from Example 3"}
par(mar=c(.1,.1,.1,.1))
plotDAG(Dset, vertex_attrs = list(size = 18, label.cex = 0.8))
 -->

## Estimation of causal effects in observational network data using `tmlenet` `R` package

We use the function `tmlenet` to perform the estimation of the mean outcome under stochastic intervention `f.gstar`, using the above sampled observed data `datO`, the network defined by the matrix `NetInd_mat` and the argument `Qform` to specify the regression model for the conditional mean of the outcome `Y`, given summary measures $W^s,A^s$.

Below we give an example of the user-specified stochastic intervention that will be provided as an input to the `tmlenet` function. The user-defined stochast intervention is a function that should always return a vector of new treatment assignments. The example below uses the shifted normal intervention, which is the same intervention defined above using the `DAG` object of the `simcausal` package. This function returns a stochastic intervention function intervening on `sA`, for given `shift` and `trunc` constants.

```{r, message=FALSE}
library(tmlenet)
options(tmlenet.verbose = FALSE)
nsamp <- 5000
datO <- sim(Dset, n = nsamp, rndseed = 54321)
NetInd_mat <- attributes(datO)$netind_cl$NetInd
nF <- attributes(datO)$netind_cl$nF
datO <- datO[,c("W1", "W2", "W3", "sA", "Y")]
print("mean(datO$Y): " %+% mean(datO$Y));
```

```{r}
create_f.gstar <- function(shift, trunc) {
  f.gstar <- function(data, ...) {
    sA.mu <- 0.98 * data[,"W1"] + 0.58 * data[,"W2"] + 0.33 * data[,"W3"]
    sA <- data[,"sA"]
    untrunc.sA.gstar <- sA + shift
    r.new.sA <- exp(shift * (untrunc.sA.gstar - sA.mu - shift / 2))
    trunc.sA.gstar <- ifelse(r.new.sA > trunc, sA, untrunc.sA.gstar)
    return(trunc.sA.gstar)
  }
  return(f.gstar)
}
```

Next, we define the (network) summary measures $W^s$ and $A^s$ for the `tmlenet` function input using a syntax that is identical to the syntax described in the `simcausal` package vignette "Simulations with Networks-Based Dependent-Data Structural Equation Models" [@R-simcausal]. The function `def.sW` is used to specify the baseline-only summary measures $W^s:=w_i(\mathbf{W})$ and the function `def.sA` is used to specify the exposure and baseline summary measures $A^s:=a_i(\mathbf{A},\mathbf{W})$. This summary measures form the basis of the `exposure model`, i.e., $P(A^s|W^s)$, where the latter probability model is defined under the true exposure mechanism $\mathbf{g}_0(\mathbf{A}|\mathbf{W})$ and the user-specified stochastic intervention $\mathbf{g}^*(\mathbf{A}^*|\mathbf{W})$, where the stochastic intervention of interest is defined with the argument `f_gstar1` of the `tmlenet` function.


```{r, results='hide', message=FALSE}
def_sA <- def.sA(sA,
                net.mean.sA = ifelse(nF > 0, rowSums(sA[[1:Kmax]])/nF, 0),
                replaceNAw0 = TRUE)

def_sW <- def.sW(W1, W2, W3) +
          def.sW(net.mean.W1 = ifelse(nF > 0, rowSums(W1[[1:Kmax]])/nF, 0),
                replaceNAw0 = TRUE)
```


One can examine the evaluation results of such summary measures by calling the helper function \texttt{eval.summaries}. Note that this function is provided purely for the user's convenience, that is, \texttt{eval.summaries} doesn't need to be called for performing estimation with the \texttt{tmlenet} function.

```{r, message=FALSE}
Kmax <- 10
trunc.const <- 4
shift.const <- 1

res <- eval.summaries(sW = def_sW, sA = def_sA, 
                      Kmax = Kmax, data = datO,
                      NETIDmat = NetInd_mat, verbose = FALSE)
names(res)
```

## Estimation example 1.

 The example below performs estimation using the default regression models for $E[Y_|A^s_i,W^s_i]$, $P_{\mathbf{g}_0}(A^s|W^s)$ and $P_{\mathbf{g}^*}(A^s|W^s)$. Note that below we are changing the default settings of the estimation procedure, specifically, we are calling the \texttt{tmlenet\_options} function and setting the parameters: \texttt{maxNperBin} to \texttt{1000} (maximum number of observations per one bin when performing the conditional histogram estimation for continuous summary in $A^s$) and \texttt{bin.method} to \texttt{"equal.mass"} (data-adaptive definition for bin cutoffs, all bin intervals will have the same number of observations and varying interval length). To learn more about all such options available for the \textbf{tmlenet} package, see `?tmlenet_options`. To see the values of the current used option parameters, call `print_tmlenet_opts()`.


```{r, message=FALSE}
tmlenet_options(maxNperBin = 1000, bin.method="equal.mass")
f.gstar <- create_f.gstar(shift = shift.const, trunc = trunc.const)
res1 <- tmlenet(data = datO, Anodes = "sA", Ynode = "Y",
                Kmax = Kmax,
                NETIDmat = NetInd_mat,
                f_gstar1 = f.gstar,
                sW = def_sW, sA = def_sA,
                optPars = list(n_MCsims = 1))

```

The output of the `tmlenet` function call is named list with 3 items:

1. `EY_gstar1` - the estimates of the mean counterfactual outcome under (stochastic) intervention function `f_gstar1` ($E_{g^*_1}[Y]$); 
2. `EY_gstar2` - the same estimates under for intervention `f_gstar2` ($E_{g^*_2}[Y]$), `NULL` if `f_gstar2` not specified;
3. `ATE` - the additive treatment effect ($E_{g^*_1}[Y] - E_{g^*_2}[Y]$) under interventions `f_gstar1` and `f_gstar2`, `NULL` if `f_gstar2` not specified.


Each of this list will contain items `estimates`, `vars`, `CIs`, `h_g0_SummariesModel` and `h_gstar_SummariesModel`.


`estimates` either provides the estimated mean outcome for the corresponding stochastic interventions `f_gstar1` or `f_gstar2` or the ATE, for the TMLE, IPTW and G-COMP estimators, as shown below:

```{r}
res1$EY_gstar1$estimates
```
The asymptotic variance estimates are provided in `vars`:

```{r}
res1$EY_gstar1$vars
```

The `alpha`-level asymptotic confidence intervals (based on the asymptotic normality of the TMLE and IPTW) are provided in `CI` (the default `alpha` is set to 0.95):

```{r}
res1$EY_gstar1$CIs
```

Finally, `h_g0_SummariesModel` and `h_gstar_SummariesModel` provide the fit objects for the exposure model $P(A^s|W^s)$, for the observed exposure mechanism $\mathbf{g}_0$ and for the stochastic intervention (either `f_gstar1` or `f_gstar2`), respectively, which could be examined as shown below:

```{r, eval=FALSE}
res1$EY_gstar1$h_g0_SummariesModel
res1$EY_gstar1$h_gstar_SummariesModel
```

## Estimation example 2.

The example below shows how to specify the regression models for the conditional mean outcome `Y` as a function of the above defined summary measures. The first model is a correctly specified model (`Qform.corr`), while the second one a misspecified model (`Qform.mis`):

```{r}
Qform.corr <- "Y ~ W1 + W2 + W3 + sA + net.mean.sA + net.mean.W1"
Qform.mis <- "Y ~ W2 + W3 + net.mean.sA"
```

Specifying the exposure regression model under observed exposure mechanism $\mathbf{g}_0$:

```{r}
hform.g0 <- "sA + net.mean.sA ~ W1 + W2 + W3 + net.mean.W1"
```

Specifying the exposure regression model under user-specified stochastic intervention $\mathbf{g}^*$ :

```{r}
hform.gstar = "sA + net.mean.sA ~ W1 + W2 + W3 + net.mean.W1"
```

```{r, message=FALSE}
res2 <- tmlenet(data = datO, Kmax = Kmax, NETIDmat = NetInd_mat,
                sW = def_sW, sA = def_sA,
                Anodes = "sA", Ynode = "Y",
                f_gstar1 = f.gstar,
                Qform = Qform.corr, hform.g0 = hform.g0, hform.gstar = hform.gstar,
                optPars = list(n_MCsims = 1))
res2$EY_gstar1$estimates
```

## Estimation example 3.

We now run the TMLE estimation for networks by calling the `tmlenet` function and providing the above defined inputs, but using the misspecified conditional outcome model defined in `Qform.mis`:

```{r, message=FALSE}
res3 <- tmlenet(data = datO, Kmax = Kmax, NETIDmat = NetInd_mat, 
                sW = def_sW, sA = def_sA, 
                Anodes = "sA", Ynode = "Y",
                f_gstar1 = f.gstar,
                Qform = Qform.mis, hform.g0 = hform.g0, hform.gstar = hform.gstar,
                optPars = list(n_MCsims = 1))
res3$EY_gstar1$estimates
```

<!-- ************************************************ -->
# Discussion
<!-- ************************************************ -->


<!-- ************************************************ -->
# Acknowledgments
<!-- ************************************************ -->

This research was supported by NIH grant R01 AI074345-07.

# References

